최적값 찾는것 2가지 : 최소값을 찾거나 최대값을 찾거나
cost(w, b)

절대값함수는 미분을 할 수 없으므로, 제곱을 취한다.

기울기의 반대방향으로 가면, cost가 최소가 되는 기울기값을 찾을 수 있다.

이걸 수식으로 형상화 한것이, w = w-(da/dw)*cost(w)



미니배치
경사하강법
과적합(Overfiting)

####오늘, 시작을 어디서 할까?####
일단, 랜덤으로 주고 시작을 한다.

정규분포 초기화의 문제점

시그모이드를 적용하면, 정규분포를 이용한 초기화에서는 w가 아무리 작아도, wx*b의 값이 발산한다.
w를 양수로 했기 때문에, wx*b가 발산했다고, 생각하여, w의 분포를 -4 ~ 4의 범위로 재조정(정규분포), sigmoid는 5보다 크면, 1, -5보다 작으면 0으로 추정(이것은 제대로 학습이 되지 않은것)

w의 크기를 더 줄이기 위해, 정규분포 N(0, 1)에 따르는값으로 w의 범위 지정 결과값들이 -5 ~ 5 사이이기 때문에, sigmoid함수의 분포가 고르게 된다.

요약 : 시그마 한 후 결과 값이 너무 크니까, w를 최소화 하자

w분포 : w가 0 ~ 1사이의 값

영상에서는 sigma(wx + b)로 처리해주어야할게, 몇만개 이상이다.
정규분포를 하면, sigmoid 쓰는게, step함수와 같은 효과를 준다.

 출력이 1이 된다.



w를 다 양수로 해서 문제가 되는것, 그래서, -4 ~ 4로 초기화를 하게 한다.
sigmoid 5보다 크면, 다 1, -5보다 작으면, 0으로 

그래서, N(0, 0.1)
대부분이 sigmoid값에서 의미가 있는, 0과1의 사이값(경사)에 분포

그래서, 2가지 초기화 방법을 이제 사용해 보겠다.

입력노드 개수와 출력노드 개수를 토대로 결정

CNN할때는, He 초기화를 사용



오버피팅
옷을 입는데, 너무 타이트하게 입는게 오버피팅
타이트하게 트레이닝 데이터에 맞춰서, 선을 따는것, 에러는 0이긴하다.
대신, 환경이 변하면, 정확도가 확 떨어진다.



언더피팅
언더피팅은 너무 헐렁하게 입는것



CNN에서는 많이 일어난다. 데이터는 적고, 학습해야할것은 많다.
네트워크가 깊을 수록 오버피팅 발생가능성이 높다.


과적합 막는법?
너 그만 공부해, 더해봤자, 안좋아지니까

공부를 하면할수록, 오히려, 안나오는 경우가 발생한다.
어느 순간, error값이 증가하면, 멈춘다.

그렇다고, validation과 training 데이터가 비슷하면, 오버피팅을 발견할 수 없다.
적당히 다른 데이터를 쓰는게 좋다.

그래도 사람들이, 오버피팅을 해결하는 좋은 방법이 없을까? 고민
그래서, 드롭아웃을 하는것
학습단계에서 랜덤으로 임의의 뉴런을 선택해 삭제해, 신호를 더 이상 다음 계층으로 전달하지 않는다.

랜덤하게 아무전략없이 뺏더니, 개선이 되더라, 그래서 쓰는것

효과 : voting 효과, Co-adaptation

Voting : 약한분류기(weak classfier)을 여러개 만든다. [학습을 끝까지 안시키고 대충 시킨다.], 대충 시킨 애들만 사용한다.

약한 분류기
약한 분류기 -> 강한분류기
약한 분류기

약한 애들을 모아, 하나의 최적화된걸 만든다. (일종의 다수결 방식이라고 보면 된다.)

최적화pdf 42쪽 참조

위가 strong classfier
아래가 weak classfier

가장 많이 쓰이는것이, 얼굴검출할때다. 의외로 간단하다.
밝고 어두운것만 가지고 판단한다.
다만, weak classfier을 수만개를 사용한다.

*이게 앙상블*

pdf 45쪽
전체 데이터에서minibatch를 사용해 얘를 가지고 각각 학습을 한다. 데이터가 달라진다.
weak classfier가 될 확률이 높아진다.

즉, dropout을 많이 해야한다. 50%이상을 해야, 좋은 효과가 나온다.

accuracy가 높은 애들은, 앙상블을 해봤자, 별 개선이 안된다. 큰 효과 없다.

다양성이 낮다.

#트레이닝에 대한 요동을 치는 이유는 0으로 만드니까, 인식률이 꾸준히 상승하지 못해서 그렇다.


Co-adaptation : 가중치가 다음과 같이 나왔다.
[72.2, 0.1, -0.02] -> [0, 0.01, -0.02]
72.2빼고, 옆의 애들은 의미가 없다. 그래서, 72.2를 0으로 만들어준다.
물론, 0.01이나, -0.02가 없어질 수도 있다. 그래서 랜덤이다.

90% 5개보다, 80% 5개가 다양성도 있고, 개선도 더 잘 될 수 있다.

앙상블을 하면, 다양성이 줄어드는 효과가 있다. 평균의 효과가 발생한다. 흔들리는 효과가 적다.



###############################
차원수가 높아지면, 선형문제와 비선형문제 경계가 애매해진다.

선형문제를 다루는 이유 : 간단하다., 차원을 높이면, 비선형문제도 해결할 수 있다.



출력값을 제한한다.
-inf ~ inf => 0 ~ 1로 변환시킨다.
              _
/   =>   _/   꼴로 만든다.

                     ____1
다시 얘를  0___/          =>  sigmoid로 만든다.

sigmoid : 중심축 0을 기준으로, -inf로 가면, 0으로 수렴, inf로 가면, 1로 수렴


경사하강법을 이용한 최적화



선형함수 조합의 한계

선형함수의 조합은 선형함수다.

f1(x) = 2x
f2(x) = 3x
f3(x) = 4x

f1(f2(f3)) = 24x

더하는것도 마찬가지

선형문제를 사용하면, 좋은 그래프가 그려져 좋지만, 선형함수는 아무리 다층을 형성해도, 단층과 똑같다.
그것이 한계점

다층을 사용해야만, 비선형문제를 풀 수 있다.